{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Flatten, LSTM, GRU, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical  \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import get_value, clip\n",
    "from tensorflow.python.eager.context import eager_mode, graph_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the Gym environnement with continuous States & discrete actions\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-sandwich",
   "metadata": {},
   "source": [
    "## Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "        \n",
    "    def __init__(self, env):\n",
    "        self.env=env #import env\n",
    "        self.state_shape=env.observation_space.shape # the state space\n",
    "        self.action_shape=env.action_space.n # the action space\n",
    "        self.gamma=0.99 # decay rate of past observations\n",
    "        self.alpha=1e-4 # learning rate in the policy gradient\n",
    "        self.lr=0.01 # learning rate in deep learning\n",
    "        self.model=self.build_agent(self.state_shape)\n",
    "            \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "    \n",
    "    def build_agent(self, s_size):\n",
    "        i=tf.keras.layers.Input(s_size)\n",
    "        l_1 = tf.keras.layers.Dense(256,activation=LeakyReLU(alpha=0.2))(i)\n",
    "        #l_2 = tf.keras.layers.BatchNormalization()(l_1)\n",
    "        l_3 = tf.keras.layers.Dense(256,activation=LeakyReLU(alpha=0.2))(l_1)\n",
    "        l_4 = tf.keras.layers.Dense(256,activation=LeakyReLU(alpha=0.2))(l_1)\n",
    "        \n",
    "        action = tf.keras.layers.Dense(5, activation=\"softmax\")(l_3)\n",
    "        critic = layers.Dense(1)(l_4)\n",
    "\n",
    "        model = tf.keras.Model(inputs=i, outputs=[action, critic])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "for i in range(10):\n",
    "    lr =lr/(1+)\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_environment = MicroGridEnv(env_config={'microgrid':buildings[0],\"testing\":False})\n",
    "env = building_environment\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules. ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10,\n",
    "    decay_rate=0.05)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(agent.lr)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "episodes = 50\n",
    "\n",
    "agent = Agent(env)\n",
    "model = agent.model\n",
    "#model.compile(optimizer= optimizer(agent.lr),loss=huber_loss)\n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "last_episode_reward =0\n",
    "episode_count = 0\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    j=0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        while not done:  # Run until solved\n",
    "            #j+=1\n",
    "            #if j%100==0:print(j,end=\" \",flush=True)\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "            \n",
    "             # Sample action from action probability distribution\n",
    "            action = np.random.choice(agent.action_shape, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        #if episode_reward-last_episode_reward < 100/(episode+1):\n",
    "            #agent.lr*=1.05\n",
    "        #elif last_episode_reward>episode_reward:\n",
    "            #agent.lr/=\n",
    "        #optimizer = tf.keras.optimizers.Adam(agent.lr)\n",
    "\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + agent.gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        \n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "        \n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        \n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "            \n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
    "        \n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    \n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "        print(loss_value)\n",
    "        #if episode_count % 10 == 0:\n",
    "        last_episode_reward=np.copy(episode_reward)\n",
    "    template = \"running reward: {:.2f} at episode {}\"\n",
    "    print(template.format(episode_reward, episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-shelf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
