{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxpy\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here\n",
    "from train_per import *\n",
    "from wrappers import NormalizedMicroGridEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perfect_train_scores = [4068.5, 13568.92, 15345.97]\n",
    "similarity_stop = 0.3\n",
    "MAX_EPISODES = 10\n",
    "\n",
    "def train(building_idx, building_env, perfect_train_score):\n",
    "    env = NormalizedMicroGridEnv(building_env)\n",
    "    #env = FlattenObservation(FrameStack(env, 24))\n",
    "\n",
    "    #writer = SummaryWriter(comment=current_time)\n",
    "\n",
    "    state_size = env.observation_space.low.size\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scores, steps = [], 0\n",
    "    scores_list = deque(maxlen=5) #Â yearly score\n",
    "\n",
    "    for e in range(MAX_EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                loss = agent.train_model()\n",
    "                #writer.add_scalar('Loss', loss, steps)\n",
    "                steps += 1\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "                agent.scheduler.step()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                scores.append(score)\n",
    "                print(\"episode:\", e, \"\\tscore:\", score, \"\\tmemory length:\",\n",
    "                        agent.memory.tree.n_entries, \"\\tepsilon:\", agent.epsilon,\n",
    "                        \"\\tlearning rate:\", agent.scheduler.get_last_lr()[0])\n",
    "\n",
    "                #writer.add_scalar('Learning rate', agent.scheduler.get_last_lr()[0], e)\n",
    "                #writer.add_scalar('Training total building cost', score, e)\n",
    "\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" + str(building_idx))\n",
    "                torch.save(agent.model, \"./save_model/per_dqn_\" +  str(building_idx) + \"_\" +\n",
    "                            current_time + \"_\" + str(e).zfill(5))\n",
    "\n",
    "                # Early stop\n",
    "                similarity = abs(score - perfect_train_score) / perfect_train_score\n",
    "                print(\"Similarity to perfect score\", similarity)\n",
    "                if similarity <= similarity_stop:\n",
    "                    print(\"Reached similarity stop of\", similarity_stop)\n",
    "                    return\n",
    "                if len(scores_list) >= scores_list.maxlen:\n",
    "                    mean = np.mean(scores_list)\n",
    "                    similarity = abs(score - mean) / abs(mean)\n",
    "                    print(\"Similarity to stable score\", similarity)\n",
    "                    if similarity <= similarity_stop:\n",
    "                        print(\"Reached stable score\")\n",
    "                        return\n",
    "                scores_list.append(score)\n",
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "for building_idx, (building_env, perfect_train_score) in enumerate(zip(building_environments, perfect_train_scores)):\n",
    "    train(building_idx, building_env, perfect_train_score)\n",
    "\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start\n",
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i], \"testing\": True}) for i in range(3)]\n",
    "\n",
    "for i, building_env in enumerate(building_environments):\n",
    "    test_done = False\n",
    "    test_score = 0\n",
    "\n",
    "    test_env = NormalizedMicroGridEnv(building_env)\n",
    "    state_size = test_env.observation_space.low.size\n",
    "    action_size = test_env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.model = torch.load('save_model/per_dqn_' + str(i))\n",
    "    agent.model.train(False)\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not test_done:\n",
    "            state = torch.from_numpy(state).float().cpu()\n",
    "            q_value = agent.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            action = int(action)\n",
    "            next_state, reward, test_done, info = test_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if test_done:\n",
    "                #writer.add_scalar('Testing total building cost', test_score, e)\n",
    "                total_cost[i] = test_score\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start\n",
    "print(test_frugality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frugality = train_frugality + test_frugality\n",
    "print(frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
