{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPBwR4VzWBae"
   },
   "source": [
    "## Submission Notebook Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6teFSe8-WBal"
   },
   "source": [
    "<h3> <font color='red'> WARNING : </font>  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEMKHrodWBam"
   },
   "source": [
    "<font color='red'> No matter which approach you've chosen, you need to re-install any custom packages you had to install to make your code work ! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MneQi5hRWBan"
   },
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1ql9JlIWBan",
    "outputId": "bce04adb-ff44-4129-b5e1-772a6edbdeb9"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Total-RD/pymgrid/\n",
    "    \n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtRQwqitWBas"
   },
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tfA-MUuoWBat"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open(\"building_1.pkl\", \"rb\") as f:\n",
    "    building_1 = pickle.load(f)\n",
    "    building_1.train_test_split()\n",
    "\n",
    "with open(\"building_2.pkl\", \"rb\") as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    building_2.train_test_split()\n",
    "\n",
    "with open(\"building_3.pkl\", \"rb\") as f:\n",
    "    building_3 = pickle.load(f)\n",
    "    building_3.train_test_split()\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz9wdURxWBax"
   },
   "source": [
    "<h2> Evaluation for \"Simple\" Reinforcement Learning based approaches <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omJ16eV3WBax"
   },
   "source": [
    " <font color='red'> <b> Be careful, the rewards returned by the Gym environment are negative ! Don't forget to multiply by -1 the total reward as the Profitability you will need to submit needs to be positive ! </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWi9L9wmWBay"
   },
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ywvicSqJWBaz"
   },
   "outputs": [],
   "source": [
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkXS75DGWBa0"
   },
   "source": [
    "<b> 2) Agent & Environment Setup before your training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iWgytWpPWBa0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment initialisation\n",
    "\"\"\"\n",
    "building_environments = []\n",
    "for i, building in enumerate(buildings):\n",
    "    env_config = {\"building\": building}\n",
    "    building_environments.append(DiscreteEnvironment.Environment(env_config))\n",
    "    \n",
    "\"\"\"\n",
    "Q-Table initialization\n",
    "\"\"\"\n",
    "def init_qtable(env, nb_action):\n",
    "    state = []\n",
    "    Q = {}\n",
    "\n",
    "    for i in range(\n",
    "        -int(env.mg.parameters[\"PV_rated_power\"] - 1),\n",
    "        int(env.mg.parameters[\"load\"] + 2),\n",
    "    ):\n",
    "        for j in np.arange(\n",
    "            round(env.mg.battery.soc_min, 1),\n",
    "            round(env.mg.battery.soc_max + 0.1, 1),\n",
    "            0.1,\n",
    "        ):\n",
    "            j = round(j, 1)\n",
    "            state.append((i, j))\n",
    "\n",
    "    # Initialize Q(s,a) at zero\n",
    "    for s in state:\n",
    "\n",
    "        Q[s] = {}\n",
    "\n",
    "        for a in range(nb_action):\n",
    "\n",
    "            Q[s][a] = 0\n",
    "\n",
    "    return Q\n",
    "\n",
    "\"\"\"\n",
    "Useful functions, epsilon_decreasing, max_dict, updated_epsilon\n",
    "\"\"\"\n",
    "def espilon_decreasing_greedy(action, epsilon, nb_action):\n",
    "\n",
    "    p = np.random.random()\n",
    "\n",
    "    if p < (1 - epsilon):\n",
    "        randomm = 0\n",
    "        return action, randomm\n",
    "\n",
    "    else:\n",
    "        randomm = 1\n",
    "        return np.random.choice(nb_action), randomm\n",
    "    \n",
    "def max_dict(d):\n",
    "\n",
    "    max_key = None\n",
    "    max_val = float(\"-inf\")\n",
    "\n",
    "    for k, v in d.items():\n",
    "\n",
    "        if v > max_val:\n",
    "\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "\n",
    "    return max_key, max_val\n",
    "\n",
    "def update_epsilon(epsilon):\n",
    "\n",
    "    epsilon = epsilon - epsilon * 0.02\n",
    "\n",
    "    if epsilon < 0.1:\n",
    "\n",
    "        epsilon = 0.1\n",
    "\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8GzY_B4TWBa0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Q_learning algo\n",
    "\"\"\"\n",
    "def training_Q_Learning(\n",
    "    building_environment,\n",
    "    horizon,\n",
    "    nb_episode=100,\n",
    "    epsilon=0.99,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    max_actions=7,\n",
    "):\n",
    "\n",
    "    nb_action = building_environment.action_space.n\n",
    "\n",
    "    Q = init_qtable(building_environment, nb_action)\n",
    "    nb_state = len(Q)\n",
    "\n",
    "    print_training = \"Training Progressing .\"\n",
    "\n",
    "    for e in range(nb_episode + 1):\n",
    "\n",
    "        value_print = (\n",
    "            \"\\r\" + print_training + \"Episode \" + str(e) + \"/\" + str(nb_episode)\n",
    "        )\n",
    "        sys.stdout.write(value_print)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        building_environment.reset(testing = False)\n",
    "\n",
    "        state = building_environment.state\n",
    "        action = max_dict(Q[state])[0]\n",
    "        action, randomm = espilon_decreasing_greedy(action, epsilon, nb_action)\n",
    "\n",
    "        for i in range(horizon):\n",
    "\n",
    "            state_, reward, done, info = building_environment.step(action)\n",
    "\n",
    "            action_ = max_dict(Q[state_])[0]\n",
    "\n",
    "            if i == horizon - 1:\n",
    "\n",
    "                Q[state][action] += alpha * (reward - Q[state][action])\n",
    "\n",
    "            else:\n",
    "\n",
    "                old_Q = Q[state][action]\n",
    "                target = reward + gamma * Q[state_][action_]\n",
    "                td_error = target - Q[state][action]\n",
    "                Q[state][action] = (1 - alpha) * Q[state][action] + alpha * td_error\n",
    "\n",
    "            state, action = state_, action_\n",
    "\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "    return Q\n",
    "\n",
    "\"\"\"\n",
    "Function that trains Qlearners over buildings\n",
    "\"\"\"\n",
    "def train_buildings(model_params_res):\n",
    "\n",
    "    QStrat = []\n",
    "    for i, building_environment in enumerate(building_environments):\n",
    "        print(f\"\\n -- Training for building {i+1} --\")\n",
    "        horizon = building_environment.mg._data_length - 2\n",
    "        Q = training_Q_Learning(\n",
    "            building_environment,\n",
    "            horizon,\n",
    "            nb_episode=model_params_res[\"nb_episode\"],\n",
    "            epsilon=model_params_res[\"epsilon\"],\n",
    "            alpha=model_params_res[\"alpha\"],\n",
    "            gamma=model_params_res[\"gamma\"],\n",
    "            max_actions=model_params_res[\"max_actions\"],\n",
    "        )\n",
    "\n",
    "        QStrat.append(Q)\n",
    "\n",
    "    return QStrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iPC3OYyTWBa1"
   },
   "outputs": [],
   "source": [
    "def get_optimal_and_rule_based(building_environment,Q):\n",
    "    \n",
    "    #optimal policy\n",
    "    action = max_dict(Q[building_environment.state])[0]\n",
    "    \n",
    "    #rule based\n",
    "    building = building_environment.mg\n",
    "    building_data = building.get_updated_values()\n",
    "    \n",
    "    load = building_data['load']\n",
    "    pv = building_data['pv']\n",
    "    capa_to_charge = building_data['capa_to_charge']\n",
    "    capa_to_dischare = building_data['capa_to_discharge']\n",
    "\n",
    "    p_disc = max(0, min(load-pv, capa_to_dischare, building.battery.p_discharge_max))\n",
    "    p_char = max(0, min(pv-load, capa_to_charge, building.battery.p_charge_max))\n",
    "\n",
    "    status = building_data['grid_status']\n",
    "    \n",
    "    if load - pv >=  0:\n",
    "        control_dict = {'battery_charge': 0,\n",
    "                    'battery_discharge': p_disc,\n",
    "                    'grid_import': 0,\n",
    "                    'grid_export':0,\n",
    "                    'pv_consummed': min(pv, load),\n",
    "                    'genset': max(0, load-pv-p_disc),\n",
    "               }\n",
    "\n",
    "    if load - pv <  0:\n",
    "        control_dict = {'battery_charge': p_char,\n",
    "                            'battery_discharge': 0,\n",
    "                            'grid_import': 0,\n",
    "                            'grid_export': 0,#abs(min(load-pv,0)),\n",
    "                            'pv_consummed': min(pv, load+p_char),\n",
    "                            'genset': 0,\n",
    "                       }\n",
    "        \n",
    "    return status, action, control_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fybb__dxWBa1"
   },
   "outputs": [],
   "source": [
    "#perform a step with optimal policy if connected of a rule if not\n",
    "def super_step(building_environment, grid_status, action, control_dict):\n",
    "    if grid_status == 1:\n",
    "        state, reward, done, info = building_environment.step(action)\n",
    "    else: \n",
    "        test0 = building_environment.reward\n",
    "        building_environment.mg.run(control_dict)\n",
    "        state, reward = building_environment.transition(), building_environment.get_reward()\n",
    "    \n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_XCrbxAnWBa1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that applies the optimal policy on a building\n",
    "\"\"\"\n",
    "def testing_Q_Learning(building_environment, Q):\n",
    "\n",
    "    building_environment.reset(testing=True)  #\n",
    "    horizon = building_environment.mg._data_length - 2  #\n",
    "\n",
    "    grid_status, action, control_dict =get_optimal_and_rule_based(building_environment, Q)\n",
    "    \n",
    "    cost = []\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(horizon):\n",
    "\n",
    "        state, reward = super_step(building_environment, grid_status, action, control_dict)\n",
    "    \n",
    "        cost.append(-reward)\n",
    "        total_cost += -reward\n",
    "        \n",
    "        grid_status, action, control_dict = get_optimal_and_rule_based(building_environment, Q)\n",
    "        \n",
    "    return total_cost, cost\n",
    "\n",
    "\"\"\"\n",
    "Function that tests our qlearners over buildings\n",
    "\"\"\"\n",
    "def test_buildings(model_params_res, QStrat):\n",
    "\n",
    "    total_costs = []\n",
    "    costs = []\n",
    "    for building_environment, Q in zip(building_environments, QStrat):\n",
    "\n",
    "        total_cost, cost = testing_Q_Learning(building_environment, Q)\n",
    "        total_costs.append(total_cost)\n",
    "\n",
    "    return model_params_res, total_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnwipaELWBa1"
   },
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L6pj3zwKckiK"
   },
   "outputs": [],
   "source": [
    "#you can set here the number of trainings you want to get a mean perf\n",
    "n_trainings = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQhHATasWBa2",
    "outputId": "79c9d95a-3e13-42e1-aa70-da7d3162287e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training number 0\n",
      "\n",
      " -- Training for building 1 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 2 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 3 --\n",
      "Training Progressing .Episode 30/30\n",
      "Training number 1\n",
      "\n",
      " -- Training for building 1 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 2 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 3 --\n",
      "Training Progressing .Episode 30/30\n",
      "Training number 2\n",
      "\n",
      " -- Training for building 1 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 2 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 3 --\n",
      "Training Progressing .Episode 30/30\n",
      "Training number 3\n",
      "\n",
      " -- Training for building 1 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 2 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 3 --\n",
      "Training Progressing .Episode 30/30\n",
      "Training number 4\n",
      "\n",
      " -- Training for building 1 --\n",
      "Training Progressing .Episode 30/30\n",
      " -- Training for building 2 --\n",
      "Training Progressing .Episode 2/30"
     ]
    }
   ],
   "source": [
    "nb_episode = 30\n",
    "epsilon = 0.99\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "max_actions = 5\n",
    "\n",
    "model_params_res = {\n",
    "    \"name\": \"classic\",\n",
    "    \"nb_episode\": nb_episode,\n",
    "    \"epsilon\": epsilon,\n",
    "    \"alpha\": alpha,\n",
    "    \"gamma\": gamma,\n",
    "    \"max_actions\": max_actions,\n",
    "}\n",
    "\n",
    "QStrats = []\n",
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "for k in range(n_trainings):\n",
    "    print(\"\\n\" + \"Training number \" + str(k))\n",
    "    QStrat = train_buildings(model_params_res)\n",
    "    QStrats.append(QStrat)\n",
    "\n",
    "train_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWbXvY3DWBa3"
   },
   "outputs": [],
   "source": [
    "#train_frugality = train_end - train_start\n",
    "mean_train_frugality = (train_end - train_start)/n_trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv4CR9t6WBa3"
   },
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_Qx8muoWBa3",
    "outputId": "966d68bc-6bf6-4cbb-8b9f-bb06baf6574d"
   },
   "outputs": [],
   "source": [
    "batch_total_costs = []\n",
    "\n",
    "test_start = time.process_time()\n",
    "\n",
    "for k in range(n_trainings):\n",
    "    print(k)\n",
    "    #model_params_res, total_costs = test_buildings(model_params_res, QStrats)\n",
    "    model_params_res, total_costs = test_buildings(model_params_res, QStrats[k])\n",
    "    \n",
    "    batch_total_costs.append(total_costs)\n",
    "    \n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ24m4yoWBa4"
   },
   "outputs": [],
   "source": [
    "#test_frugality = test_end - test_start\n",
    "mean_test_frugality = (test_end - test_start)/n_trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBjuF4QEWBa6"
   },
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSPRLsPkWBa6",
    "outputId": "5209208c-aa70-4337-9054-576748367cde"
   },
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : np.mean([tot[0] for tot in batch_total_costs]),\n",
    "    \"building_2_performance\" : np.mean([tot[1] for tot in batch_total_costs]),\n",
    "    \"building_3_performance\" : np.mean([tot[2] for tot in batch_total_costs]),\n",
    "    \"frugality\" : mean_train_frugality + mean_test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "evaluation_notebook_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
