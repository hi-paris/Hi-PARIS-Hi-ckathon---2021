{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYPj3njI0Pif"
   },
   "outputs": [],
   "source": [
    "# Install pymgrid\n",
    "!pip install git+https://github.com/Total-RD/pymgrid/\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqChWIKtZALb"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYsDtDts0XxA"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import time # Necessary to evaluate frugality\n",
    "import json # Necessary to export your results\n",
    "import pickle # Necessary to load the data\n",
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JowPXJENcwqp"
   },
   "outputs": [],
   "source": [
    "# Open data\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "    building_1.train_test_split()\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    building_2.train_test_split()\n",
    "\n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "    building_3.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLpQNcT6HVna"
   },
   "outputs": [],
   "source": [
    "# Group building\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA3jYA3pA3qz"
   },
   "outputs": [],
   "source": [
    "# Global hyperparameters\n",
    "def make_hyperparameters(C = 5,\n",
    "                         alpha = None,\n",
    "                         omega = 0.7,\n",
    "                         discount_factor = 0.99,\n",
    "                         epsilon = 0.99,\n",
    "                         epsilon_min = 0.1,\n",
    "                         epsilon_decay = 0.02,\n",
    "                         epsilon_expo = False,\n",
    "                         train_days = 4,\n",
    "                         train_episodes = 200,\n",
    "                         train_episodes_decay = 30,\n",
    "                         max_steps = 24):\n",
    "    return {\n",
    "          \"C\" : C,\n",
    "          \"alpha\" : alpha,\n",
    "          \"omega\" : omega,\n",
    "          \"discount_factor\" : discount_factor,\n",
    "          \"epsilon\" : epsilon,\n",
    "          \"epsilon_min\" : epsilon_min,\n",
    "          \"epsilon_decay\" : epsilon_decay,\n",
    "          \"epsilon_expo\" : epsilon_expo,\n",
    "          \"train_days\" : train_days,\n",
    "          \"train_episodes\" : train_episodes,\n",
    "          \"train_episodes_decay\" : train_episodes_decay,\n",
    "          \"max_steps\" : max_steps\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYHpOc4DJ-Os"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "def test(env, QA, QB, testing=False):\n",
    "    state = env.reset(testing=testing)\n",
    "    done = False\n",
    "    total_cost = 0\n",
    "    while not done:\n",
    "        actionA = np.argmax(QA[state])\n",
    "        actionB = np.argmax(QB[state])\n",
    "        if QA[state][actionA] > QB[state][actionB]:\n",
    "            action = actionA\n",
    "        else:\n",
    "            action = actionB\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_cost += reward\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yFexh4SJEdW"
   },
   "outputs": [],
   "source": [
    "# Test ML model\n",
    "def test_ml(env, clf, testing=False):\n",
    "    state = env.reset(testing=testing)\n",
    "    done = False\n",
    "    total_cost = 0\n",
    "    while not done:\n",
    "        action = clf.predict(np.array(state).reshape(1, 2))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_cost += reward\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atD-m5DBDxGC"
   },
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "\n",
    "def train(p, env, debug=False):\n",
    "\n",
    "    # Display the state space\n",
    "    if debug:\n",
    "        print(\"Action Space {}\".format(env.action_space))\n",
    "        print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "    # Intialize the Q table\n",
    "    QA, QB = {}, {}\n",
    "\n",
    "    # Reset discoveries\n",
    "    QA_count, QB_count = {}, {}\n",
    "\n",
    "    # Use the first reward to initialize\n",
    "    C = p[\"C\"]\n",
    "\n",
    "    # Initialize epsilon and alpha\n",
    "    epsilon = p[\"epsilon\"]\n",
    "\n",
    "    # Custom alpha decay (https://www.jmlr.org/papers/volume5/evendar03a/evendar03a.pdf)\n",
    "    \"\"\"def get_alpha(s,a):\n",
    "        if (s in Q_count) and (Q_count[s][a] > 0):\n",
    "            return 1.0 / np.power(Q_count[s][a], p[\"omega\"])\n",
    "        else:\n",
    "            return 0.0\"\"\"\n",
    "    \n",
    "    def get_alpha(s,a, Q_count):\n",
    "        if (s in Q_count) and (Q_count[s][a] > 0):\n",
    "            return 1.0 / np.power(Q_count[s][a], p[\"omega\"])\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # Train on different days\n",
    "    training_rewards = []\n",
    "\n",
    "    train_episodes = p[\"train_episodes\"]\n",
    "    for day in range(p[\"train_days\"]):\n",
    "        # print(\"Day {}/{}\".format(day+1,p[\"train_days\"]))\n",
    "\n",
    "        # Creating lists to keep track of reward and epsilon values\n",
    "        day_training_rewards = []\n",
    "\n",
    "        for episode in range(train_episodes):\n",
    "            # print(\"Episode {}/{}\".format(episode+1,train_episodes))\n",
    "\n",
    "            #Reseting the environment each time as per requirement\n",
    "            state = tuple(env.reset())\n",
    "            if not (state in QA):\n",
    "                QA[state] = [C for i in range(env.Na)]\n",
    "                QB[state] = [C for i in range(env.Na)]\n",
    "            if not (state in QA_count):\n",
    "                QA_count[state] = [0 for i in range(env.Na)]\n",
    "                QB_count[state] = [0 for i in range(env.Na)]\n",
    "\n",
    "            # Starting the tracker for the rewards\n",
    "            total_training_rewards = 0\n",
    "\n",
    "            # Choosing an action given the states based on a random number\n",
    "            exp_exp_tradeoff = np.random.random()\n",
    "\n",
    "            ### STEP 2: SECOND option for choosing the initial action - exploit\n",
    "            # If the random number is larger than epsilon: employing exploitation\n",
    "            # and selecting best action\n",
    "            if exp_exp_tradeoff < (1 - epsilon):\n",
    "                actionA = np.argmax(QA[state])\n",
    "                actionB = np.argmax(QB[state])\n",
    "                if QA[state][actionA] > QB[state][actionB]:\n",
    "                    action = actionA\n",
    "                else:\n",
    "                    action = actionB\n",
    "            ### STEP 2: FIRST option for choosing the initial action - explore\n",
    "            # Otherwise, employing exploration: choosing a random action\n",
    "            else:\n",
    "                action = np.random.choice(env.Na) # env.action_space.sample()\n",
    "\n",
    "            for step in range(p[\"max_steps\"]):\n",
    "\n",
    "                ### STEPs 3 & 4: performing the action and getting the reward\n",
    "                # Taking the action and getting the reward and outcome state\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "                new_state = tuple(new_state)\n",
    "\n",
    "                ### STEP 5: update the Q-table\n",
    "\n",
    "                # Check if values are in the table\n",
    "                if not (state in QA):\n",
    "                    QA[state] = [C for i in range(env.Na)]\n",
    "                    QB[state] = [C for i in range(env.Na)]\n",
    "                if not (state in QA_count):\n",
    "                    QA_count[state] = [0 for i in range(env.Na)]\n",
    "                    QB_count[state] = [0 for i in range(env.Na)]\n",
    "                if not (new_state in QA):\n",
    "                    QA[new_state] = [C for i in range(env.Na)]\n",
    "                    QB[new_state] = [C for i in range(env.Na)]\n",
    "\n",
    "                # Set dynamic alpha\n",
    "                if p[\"alpha\"]:\n",
    "                    alpha = p[\"alpha\"]\n",
    "\n",
    "                # 50% chance to update QA (or QB)\n",
    "                choice = np.random.random()\n",
    "\n",
    "                # Updating Q's\n",
    "                if choice >= 0.5:\n",
    "                    new_action = np.argmax(QA[new_state])\n",
    "\n",
    "                    if not p[\"alpha\"]:\n",
    "                        alpha = get_alpha(new_state, action, QA_count)\n",
    "\n",
    "                    # Updating the Q-table using the Bellman equation\n",
    "                    if step == p[\"max_steps\"] - 1:\n",
    "                        QA[state][action] += alpha * (reward - QA[state][action])\n",
    "                    else:\n",
    "                        QA[state][action] = (1 - alpha) * QA[state][action] + alpha * (reward + p[\"discount_factor\"] * QA[new_state][new_action] - QA[state][action])\n",
    "                    QA_count[state][action] += 1\n",
    "                  \n",
    "                elif choice < 0.5:\n",
    "                    new_action = np.argmax(QB[new_state])\n",
    "\n",
    "                    if not p[\"alpha\"]:\n",
    "                        alpha = get_alpha(new_state, action, QB_count)\n",
    "\n",
    "                    # Updating the Q-table using the Bellman equation\n",
    "                    if step == p[\"max_steps\"] - 1:\n",
    "                        QB[state][action] += alpha * (reward - QB[state][action])\n",
    "                    else:\n",
    "                        QB[state][action] = (1 - alpha) * QB[state][action] + alpha * (reward + p[\"discount_factor\"] * QB[new_state][new_action] - QB[state][action])\n",
    "                    QB_count[state][action] += 1\n",
    "\n",
    "                # Increasing our total reward and updating the state\n",
    "                total_training_rewards -= reward\n",
    "                state = new_state\n",
    "                action = new_action\n",
    "\n",
    "                # Ending the episode\n",
    "                if done == True:\n",
    "                    if debug:\n",
    "                        print (\"Total reward for episode {}: {}\".format(episode, total_training_rewards))\n",
    "                    break\n",
    "\n",
    "            # Cutting down on exploration by reducing the epsilon\n",
    "            if p[\"epsilon_expo\"]:\n",
    "              epsilon = p[\"epsilon_min\"] + (p[\"epsilon\"] - p[\"epsilon_min\"]) * np.exp(-p[\"epsilon_decay\"] * episode)\n",
    "            else:\n",
    "              epsilon = max(p[\"epsilon_min\"], epsilon - epsilon * p[\"epsilon_decay\"])\n",
    "\n",
    "            # Adding the total reward and reduced epsilon values\n",
    "            day_training_rewards.append(total_training_rewards)\n",
    "\n",
    "        # Decrease the number of steps\n",
    "        train_episodes -= p[\"train_episodes_decay\"]\n",
    "        training_rewards += day_training_rewards\n",
    "\n",
    "        if debug:\n",
    "            print (\"Training score over time (Day {}): \".format(day) + str(sum(training_rewards)/train_episodes))\n",
    "\n",
    "    return training_rewards, QA, QB, QA_count, QB_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHBXLyjEJXTH"
   },
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "def build_clf(p, QA, QB, QA_count, QB_count):\n",
    "\n",
    "  # Build dataset\n",
    "  X = []\n",
    "  y = []\n",
    "  for s in QA.keys():\n",
    "    actionA = np.argmax(QA[s])\n",
    "    actionB = np.argmax(QB[s])\n",
    "\n",
    "    # Choose the best action\n",
    "    if QA[s][actionA] > QB[s][actionB]:\n",
    "        action, orig = actionA, 1\n",
    "    elif QA[s][actionA] < QB[s][actionB]:\n",
    "        action, orig = actionB, 0\n",
    "    else:\n",
    "        action, orig = random.choice([(actionA,1), (actionB,0)])\n",
    "\n",
    "    # If unitialized, choose the another\n",
    "    if (orig == 1) and not ((s in QA_count and QA_count[s][action] > 0) or (QA[s][action] != p['C'])):\n",
    "        action, orig = actionB, 0\n",
    "    elif (orig == 0) and not ((s in QB_count and QB_count[s][action] > 0) or (QB[s][action] != p['C'])):\n",
    "        action, orig = actionA, 1    \n",
    "\n",
    "    # Append to dataset\n",
    "    if (orig == 1) and ((s in QA_count and QA_count[s][action] > 0) or (QA[s][action] != p['C'])):\n",
    "      y.append(action)\n",
    "      X.append(np.array(s))\n",
    "    if (orig == 0) and ((s in QB_count and QB_count[s][action] > 0) or (QB[s][action] != p['C'])):\n",
    "      y.append(action)\n",
    "      X.append(np.array(s))\n",
    "\n",
    "  # Build and train model\n",
    "  clf = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "  clf.fit(X, y)\n",
    "\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_T0U5VzLurM"
   },
   "outputs": [],
   "source": [
    "# Full benchmark\n",
    "import time\n",
    "\n",
    "def benchmark(p):\n",
    "  building_environments = [DiscreteEnvironment.Environment(env_config={'building':buildings[i]}) for i in range(3)]\n",
    "\n",
    "  # Train on the first building\n",
    "  clfs = [None, None, None]\n",
    "  train_start = time.process_time()\n",
    "  for i,building_env in enumerate(building_environments):\n",
    "    training_rewards, QA, QB, QA_count, QB_count = train(p, building_env)\n",
    "    clfs[i] = build_clf(p, QA, QB, QA_count, QB_count)\n",
    "  train_end = time.process_time()\n",
    "\n",
    "  # Test our model on train data\n",
    "  scores_train = [0,0,0]\n",
    "  for i,building_env in enumerate(building_environments):\n",
    "    scores_train[i] = -1 * test_ml(building_env, clfs[i], testing=False)\n",
    "\n",
    "  # Test our model on test data\n",
    "  scores_test = [0,0,0]\n",
    "  test_start = time.process_time()\n",
    "  for i,building_env in enumerate(building_environments):\n",
    "    scores_test[i] = -1 * test_ml(building_env, clfs[i], testing=True)\n",
    "  test_end = time.process_time()\n",
    "\n",
    "  # Usefull quantities\n",
    "  train_frugality = train_end - train_start\n",
    "  test_frugality = test_end - test_start\n",
    "  frugality = train_frugality + test_frugality\n",
    "  total_cost = scores_test\n",
    "\n",
    "  # Make results\n",
    "  final_results = {\n",
    "      \"building_1_performance\" : total_cost[0],\n",
    "      \"building_2_performance\" : total_cost[1],\n",
    "      \"building_3_performance\" : total_cost[2],\n",
    "      \"frugality\" : frugality,\n",
    "  }\n",
    "\n",
    "  return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark our model\n",
    "\n",
    "best_param = {\n",
    "     'C': 5,\n",
    "     'alpha': 0.22631578947368422,\n",
    "     'discount_factor': 1.0,\n",
    "     'epsilon': 0.99,\n",
    "     'epsilon_decay': 4.894736842105263,\n",
    "     'epsilon_expo': True,\n",
    "     'epsilon_min': 0.1,\n",
    "     'max_steps': 24,\n",
    "     'omega': 0.7289473684210527,\n",
    "     'train_days': 1,\n",
    "     'train_episodes': 30,\n",
    "     'train_episodes_decay': 30\n",
    "}\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "final_results = benchmark(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook - Louis - DoubleQL - Hyperparam.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
