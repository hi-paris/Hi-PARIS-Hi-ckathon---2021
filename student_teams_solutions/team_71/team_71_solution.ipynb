{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of necessary packages\n",
    "\n",
    "import time # Necessary to evaluate frugality\n",
    "import json # Necessary to export your results\n",
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "## Other packages\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-stockholm",
   "metadata": {},
   "source": [
    "## Setup for environment and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]\n",
    "\n",
    "building_environments = [\n",
    "    DiscreteEnvironment.Environment(env_config={'building':buildings[i]}) for i in range(3)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table initialisation\n",
    "\n",
    "def init_qtable(env, nb_action):\n",
    "    \n",
    "    state = []\n",
    "    Q = {}\n",
    "\n",
    "    for i in range(-int(env.mg.parameters['PV_rated_power']-1),int(env.mg.parameters['load']+2)):\n",
    "        \n",
    "        for j in np.arange(round(env.mg.battery.soc_min,1),round(env.mg.battery.soc_max+0.1,1),0.1):\n",
    "            \n",
    "            j = round(j,1)\n",
    "            state.append((i,j)) \n",
    "\n",
    "    #Initialize Q(s,a) at zero\n",
    "    for s in state:\n",
    "\n",
    "        Q[s] = {}\n",
    "\n",
    "        for a in range(nb_action):\n",
    "\n",
    "            Q[s][a] = 0\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent \n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, actions, env, epsilon):\n",
    "        # actions = [0, 1, 2, 3, 4]\n",
    "        self.actions = actions\n",
    "        self.env = env\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = init_qtable(self.env,len(self.actions))\n",
    "    \n",
    "    #we make an epsilon-greedy approach for the training\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action \n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function table\n",
    "            current_state = self.q_table[state]\n",
    "            action = self.max_dict(current_state)[0]\n",
    "        return action\n",
    "        \n",
    "    # update q function\n",
    "    def updateQ(self, state, action, reward, next_state):\n",
    "        current_q = self.q_table[state][action]\n",
    "        new_q = reward + self.gamma * max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.alpha * (new_q - current_q)\n",
    "    \n",
    "    #defining max_dict for our q-table\n",
    "    def max_dict(self, d):\n",
    "\n",
    "        max_key = None\n",
    "        max_val = float('-inf')\n",
    "\n",
    "\n",
    "        for k,v in d.items():\n",
    "\n",
    "            if v > max_val:\n",
    "\n",
    "                max_val = v\n",
    "                max_key = k\n",
    "\n",
    "        return max_key, max_val\n",
    "    \n",
    "    def update_epsilon(self, ep):\n",
    "        ep = ep - ep *0.05\n",
    "        if ep < 0.01:  \n",
    "            ep = 0.01\n",
    "        self.epsilon = ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-reproduction",
   "metadata": {},
   "source": [
    "## Training of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "env0 = building_environments[0]\n",
    "agent0 = QAgent(actions=[0,1,2,3,4], env=env0, epsilon = 0.99)\n",
    "cost_list0 = []\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env0.reset()\n",
    "\n",
    "    while True:\n",
    "        # take action and proceed one step in the environment\n",
    "        action = agent0.choose_action(state)\n",
    "        next_state, reward, done, _ = env0.step(action)\n",
    "        \n",
    "        # with sample <s,a,r,s'>, agent learns new q function\n",
    "        agent0.updateQ(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # if episode ends, then break\n",
    "        if done:\n",
    "            cost_list0.append(env0.get_cost())\n",
    "            break\n",
    "            \n",
    "            train_start = time.process_time()\n",
    "\n",
    "env1 = building_environments[1]\n",
    "agent1 = QAgent(actions=[0,1,2,3,4], env=env1, epsilon = 0.99)\n",
    "cost_list1 = []\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env1.reset()\n",
    "\n",
    "    while True:\n",
    "        # take action and proceed one step in the environment\n",
    "        action = agent1.choose_action(state)\n",
    "        next_state, reward, done, _ = env1.step(action)\n",
    "        \n",
    "        # with sample <s,a,r,s'>, agent learns new q function\n",
    "        agent1.updateQ(state, action, reward, next_state)\n",
    "        agent1.update_epsilon(agent1.epsilon)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # if episode ends, then break\n",
    "        if done:\n",
    "            cost_list1.append(env1.get_cost())\n",
    "            break\n",
    "\n",
    "env2 = building_environments[2]\n",
    "agent2 = QAgent(actions=[0,1,2,3,4], env=env2, epsilon = 0.99)\n",
    "cost_list2 = []\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env2.reset()\n",
    "\n",
    "    while True:\n",
    "        # take action and proceed one step in the environment\n",
    "        action = agent2.choose_action(state)\n",
    "        next_state, reward, done, _ = env2.step(action)\n",
    "        \n",
    "        # with sample <s,a,r,s'>, agent learns new q function\n",
    "        agent2.updateQ(state, action, reward, next_state)\n",
    "        agent2.update_epsilon(agent2.epsilon)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # if episode ends, then break\n",
    "        if done:\n",
    "            cost_list2.append(env2.get_cost())\n",
    "            break\n",
    "\n",
    "train_end = time.process_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frugality = train_end - train_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-showcase",
   "metadata": {},
   "source": [
    "### Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "agent_list = [agent0, agent1, agent2]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "    agent = agent_list[i]\n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.max_dict(agent.q_table[obs])[0]\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]+=reward\n",
    "\n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frugality = test_end - test_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : -1*total_cost[0],\n",
    "    \"building_2_performance\" : -1*total_cost[1],\n",
    "    \"building_3_performance\" : -1*total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
