{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color='red'> WARNING : </font>  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> No matter which approach you've chosen, you need to re-install any custom packages you had to install to make your code work ! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Total-RD/pymgrid/\n",
    "## Other packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "    building_1.train_test_split()\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    building_2.train_test_split()\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "    building_3.train_test_split()\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation for \"Simple\" Reinforcement Learning based approaches <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'> <b> Be careful, the rewards returned by the Gym environment are negative ! Don't forget to multiply by -1 the total reward as the Profitability you will need to submit needs to be positive ! </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "import json # Necessary to export your results\n",
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment\n",
    "import numpy as np\n",
    "\n",
    "## Other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Agent & Environment Setup before your training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_environments = [\n",
    "    DiscreteEnvironment.Environment(env_config={'building':buildings[i]}) for i in range(3)\n",
    "]\n",
    "\"\"\"\n",
    "Agent, potential Q-Table & other necessary setup code here \n",
    "\"\"\"\n",
    "\n",
    "def init_qtable(env, nb_action):\n",
    "    \"\"\"Compulsory initialization function for Q_table\"\"\"\n",
    "    state = []\n",
    "    Q = {}\n",
    "\n",
    "    for i in range(-int(env.mg.parameters['PV_rated_power']-1), int(env.mg.parameters['load']+2)):\n",
    "        for j in np.arange(round(env.mg.battery.soc_min, 1), round(env.mg.battery.soc_max+0.1, 1), 0.1):\n",
    "\n",
    "            j = round(j, 1)\n",
    "            state.append((i, j))\n",
    "    # Initialize Q(s,a) at zero\n",
    "    for s in state:\n",
    "        Q[s] = {}\n",
    "\n",
    "        for a in range(nb_action):\n",
    "\n",
    "            Q[s][a] = 0\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def choose_action_greedy(Q_table, state):\n",
    "    \"\"\"A function for choosing an action from the q-table given a certain state: greedy\n",
    "    \n",
    "    This is used for policy evaluation.\n",
    "    \"\"\"\n",
    "    nb_actions = len(Q_table[state].keys())\n",
    "    actions = list(range(nb_actions))\n",
    "    action_values = [Q_table[state][a] for a in actions]\n",
    "    action = np.argmax(action_values)\n",
    "    return action\n",
    "\n",
    "\n",
    "def choose_action_boltzmann(Q_table, state):\n",
    "    \"\"\"A function for choosing an action from the q-table given a certain state: Boltzmann\n",
    "    \n",
    "    This is used for policy optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_actions = len(Q_table[state].keys())\n",
    "    actions = list(range(nb_actions))\n",
    "    action_values = [Q_table[state][a] for a in actions]\n",
    "    # Softmax\n",
    "    if max(action_values) < -300:  # avoid underflow in softmax\n",
    "        # All actions are bad, use random choice\n",
    "        softmax = np.ones(nb_actions) / nb_actions\n",
    "    else:\n",
    "        exp_action_values = np.exp(action_values)\n",
    "        softmax = exp_action_values / np.sum(exp_action_values)\n",
    "    action = np.random.choice(actions, p=softmax)\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_q(Q_table, state, action, reward, new_state, learning_rate, discount):\n",
    "    \"\"\"TD update rule for Q\"\"\"\n",
    "    nb_actions = len(Q_table[state].keys())\n",
    "    actions = list(range(nb_actions))\n",
    "    max_Q = max([Q_table[state][a] for a in actions])\n",
    "    td_update = reward + discount * max_Q - Q_table[state][action]\n",
    "    new_qsa = (1 - learning_rate) * Q_table[state][action] + learning_rate * td_update\n",
    "    return new_qsa\n",
    "\n",
    "\n",
    "def train_q_table(building, n_epis=5, alpha=0.6, gamma=1):\n",
    "    \"\"\"Train a Q_table for the given building\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    building: pymgrid.Microgrid.Microgrid\n",
    "        The microgrid object to train the policy on\n",
    "    n_epis: int\n",
    "        The number of episodes\n",
    "    alpha: float\n",
    "        The TD learning rate\n",
    "    gamma: float\n",
    "        The discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q_table: dict\n",
    "        The trained Q_table defining the agent strategy\n",
    "    rew_hist: list(float)\n",
    "        History of the cumulative reqards otained during the training phase\n",
    "    \"\"\"\n",
    "    \n",
    "    env_config = {'building': building}\n",
    "    building_env = DiscreteEnvironment.Environment(env_config)\n",
    "    \n",
    "    nb_actions = building_env.Na\n",
    "    \n",
    "    Q_table = init_qtable(building_env, nb_action=nb_actions)\n",
    "    rew_hist = [] # reward history during episodes\n",
    "\n",
    "\n",
    "    for i in range(n_epis):\n",
    "        # Reset environment\n",
    "        s = building_env.reset(testing=False)\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        #The Q-Table learning algorithm\n",
    "        while not done:\n",
    "            # Choose action from Q table\n",
    "            try:\n",
    "                a = choose_action_boltzmann(Q_table, s)\n",
    "            except ValueError:\n",
    "                pass\n",
    "                # print(s, Q_table[s])\n",
    "            #Get new state & reward from environment\n",
    "            s1, r, done, _ = building_env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q_table[s][a] = update_q(Q_table, s, a, r, s1, alpha, gamma)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "        rew_hist.append(rAll)\n",
    "    \n",
    "    return Q_table, rew_hist\n",
    "\n",
    "\n",
    "def evaluate_policy(building, Q_table):\n",
    "    \"\"\"Runs the policy on the building and returns the cost on the test set\"\"\"\n",
    "    \n",
    "    env_config = {'building': building}\n",
    "    building_env = DiscreteEnvironment.Environment(env_config)\n",
    "    \n",
    "    nb_actions = building_env.Na\n",
    "    \n",
    "    s = building_env.reset(testing=True)\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    #The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # Choose action from Q table\n",
    "        a = choose_action_greedy(Q_table, s)\n",
    "        #Get new state & reward from environment\n",
    "        s1, r, done, _ = building_env.step(a)\n",
    "        \n",
    "        rAll += r\n",
    "        s = s1\n",
    "    \n",
    "    return -rAll\n",
    "\n",
    "\n",
    "def average_Q_tables(Q_table_list):\n",
    "    \"\"\"Takes list of Q_tables and returns the averaged Q_table.\n",
    "    \n",
    "    Used for building 3.\n",
    "    \"\"\"\n",
    "    Q_table_avg = {}\n",
    "    # Assume all Q_tables have identical structure\n",
    "    for state in Q_tables_list[0].keys():\n",
    "        Q_table_avg[state] = {}\n",
    "        for act in Q_tables_list[0][state].keys():\n",
    "            Q_table_avg[state][act] = np.mean([Q[state][act] for Q in Q_table_list])\n",
    "    \n",
    "    return Q_table_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "\"\"\"\n",
    "Training code\n",
    "\"\"\"\n",
    "\n",
    "Q_table_results = {\n",
    "    \"building0\": None,\n",
    "    \"building1\": None,\n",
    "    \"building2\": None,\n",
    "}\n",
    "\n",
    "# first Building\n",
    "Q_table_results[\"building0\"], rew_hist = train_q_table(buildings[0], n_epis=25, alpha=0.6)\n",
    "\n",
    "# second Building\n",
    "Q_table_results[\"building1\"], rew_hist = train_q_table(buildings[1], n_epis=25, alpha=0.6)\n",
    "\n",
    "# Third building\n",
    "Q_table_results[\"building2\"], rew_hist = train_q_table(buildings[2], n_epis=10, alpha=0.4)\n",
    "\n",
    "train_end = time.process_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frugality = train_end - train_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an example for a Random Agent \n",
    "\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "  mean profitability and mean frugality\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "    Q_table = Q_table_results[f\"building{i}\"]\n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        # action = np.random.randint(building_env.action_space.n)\n",
    "        action = choose_action_greedy(Q_table, obs)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]+= reward\n",
    "\n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frugality = test_end - test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : -total_cost[0],\n",
    "    \"building_2_performance\" : -total_cost[1],\n",
    "    \"building_3_performance\" : -total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
