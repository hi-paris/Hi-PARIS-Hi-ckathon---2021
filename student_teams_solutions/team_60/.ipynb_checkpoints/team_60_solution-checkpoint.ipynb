{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "guided-fluid",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-cornell",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Total-RD/pymgrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1\n",
    "#If not work try installing tensorflow 2 version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-insulation",
   "metadata": {},
   "source": [
    "## Datas loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-photography",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent(state_shape, action_shape):\n",
    "    \"\"\" The agent maps X-states to Y-actions\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    init = tf.keras.initializers.HeUniform()\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_qs(model, state, step):\n",
    "    return model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "\n",
    "def train(building_environment_contin, replay_memory, model, target_model, done):\n",
    "# An episode a full game\n",
    "    train_episodes = 1\n",
    "\n",
    "    learning_rate = 0.7 # Learning rate\n",
    "    discount_factor = 0.618\n",
    "\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch_size = 64 * 2\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([encode_observation(transition[0], building_environment_contin.observation_space.shape) for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states)\n",
    "    new_current_states = np.array([encode_observation(transition[3], building_environment_contin.observation_space.shape) for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(encode_observation(observation, building_environment_contin.observation_space.shape))\n",
    "        Y.append(current_qs)\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n",
    "\n",
    "def encode_observation(observation, n_dims):\n",
    "    return observation\n",
    "\n",
    "def get_model(building_environment_contin):\n",
    "    print(\"Action Space: {}\".format(building_environment_contin.action_space))\n",
    "    print(\"State space: {}\".format(building_environment_contin.observation_space))\n",
    "\n",
    "\n",
    "    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
    "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
    "    decay = 0.01\n",
    "\n",
    "    # Initialize the Target and Main models\n",
    "    model = agent(building_environment_contin.observation_space.shape, building_environment_contin.action_space.n)\n",
    "    target_model = agent(building_environment_contin.observation_space.shape, building_environment_contin.action_space.n)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "\n",
    "    # X = states, y = actions\n",
    "    X = []\n",
    "    y = []\n",
    "    train_episodes=1\n",
    "    steps_to_update_target_model = 0\n",
    "    train_start = time.process_time()\n",
    "    for episode in range(train_episodes):\n",
    "        total_training_rewards = 0\n",
    "        observation = building_environment_contin.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps_to_update_target_model += 1\n",
    "            #if True:\n",
    "                #building_environment_contin.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "            # Explore using Exploration Strategy\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                action = building_environment_contin.action_space.sample()\n",
    "            else:\n",
    "                # Exploit best known action\n",
    "                encoded = encode_observation(observation, building_environment_contin.observation_space.shape[0])\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                predicted = model.predict(encoded_reshaped).flatten()\n",
    "                action = np.argmax(predicted)\n",
    "            new_observation, reward, done, info = building_environment_contin.step(action)\n",
    "            replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "            #  Update the Main Network \n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                train(building_environment_contin, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                total_training_rewards=total_training_rewards*-1\n",
    "                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    print('Copying main network weights to the target network weights')\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        train_end = time.process_time()\n",
    "        train_frugality = train_end - train_start\n",
    "        return target_model,total_training_rewards,train_frugality\n",
    "    #env.close()\n",
    "\n",
    "building_train_environments_cont = [MicroGridEnv({'microgrid':building,'testing':False}) for building in buildings]\n",
    "model1,total_training_reward,train_frugality=get_model(building_train_environments_cont[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2,total_training_reward1,train_frugality1=fine_training_model(model1,building_train_environments_cont[1])\n",
    "model2,total_training_reward1,train_frugality1=get_model(building_train_environments_cont[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3,total_training_reward2,train_frugality2=fine_training_model(model1,building_train_environments_cont[1])\n",
    "model3,total_training_reward2,train_frugality2=get_model(building_train_environments_cont[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frugality=train_frugality+train_frugality1+train_frugality2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-guide",
   "metadata": {},
   "source": [
    "## Getting and exporting test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "model_list=[model1,model2,model3]\n",
    "for i,building_env in enumerate(building_train_environments_cont):\n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        encoded = encode_observation(obs, building_env.observation_space.shape[0])\n",
    "        encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "        predicted = model_list[i].predict(encoded_reshaped).flatten()\n",
    "        action = np.argmax(predicted)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]+=reward\n",
    "\n",
    "test_end = time.process_time()\n",
    "test_frugality = test_end - test_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0]*-1,\n",
    "    \"building_2_performance\" : total_cost[1]*-1,\n",
    "    \"building_3_performance\" : total_cost[2]*-1,\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
