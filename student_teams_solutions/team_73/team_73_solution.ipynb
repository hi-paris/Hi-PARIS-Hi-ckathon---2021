{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Total-RD/pymgrid/\n",
    "## Other packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "The buildings mentionned below are specific to the hackathon and are not available in this repo.\n",
    "You can replace them with any MicroGrid object generated from pymgrid\n",
    "\"\"\"\n",
    "\n",
    "with open('building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "    building_1.train_test_split()\n",
    "\n",
    "with open('building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    building_2.train_test_split()\n",
    "\n",
    "    \n",
    "with open('building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "    building_3.train_test_split()\n",
    "\n",
    "    \n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Agent & Environment Setup before your training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "\n",
    "#Class that we use to optimize our model of agents\n",
    "\n",
    "class OPT:\n",
    "    def __init__(self, Nboucle, nb_steps, env1, env2, env3):\n",
    "        \n",
    "        \"\"\"It initializes the models for the agents and the agents itself. \n",
    "    This object can be opitimized thanks to an object method with the hyperparameters\n",
    "    Nboucle and nb_steps that are self explanatory.\n",
    "    \"\"\"\n",
    "        #Hyperparameters\n",
    "        self.Nboucle = Nboucle\n",
    "        self.nb_steps =  nb_steps\n",
    "        \n",
    "        #Model for building 1\n",
    "        self.env1 = env1\n",
    "        states1 = env1.observation_space.shape\n",
    "        actions1 = env1.action_space.n\n",
    "        self.model1 = self.build_model12(states1, actions1)\n",
    "        self.dqn1 = self.build_agent(self.model1, actions1)\n",
    "        \n",
    "        #Model for building 2        \n",
    "        self.env2 = env2\n",
    "        self.model2 = self.build_model12(states1, actions1)\n",
    "        self.dqn2 = self.build_agent(self.model2, actions1)\n",
    "        \n",
    "        #Model for building 3\n",
    "        self.env3 = env3        \n",
    "        states3 = env3.observation_space.shape\n",
    "        actions3 = env3.action_space.n\n",
    "        self.model3 = self.build_model3(states3, actions3)\n",
    "        self.dqn3 = self.build_agent(self.model3, actions3)\n",
    "        \n",
    "        \n",
    "    def optBuilding(self,lr_init, decay_steps, Nboucle, index, model, dqn, env):\n",
    "        \"\"\"It optimizes the agent 'dqn' of the building 'index' with the hyperparameters in argument\"\"\"\n",
    "        #Initial learning rate\n",
    "        lr = tf.keras.optimizers.schedules.ExponentialDecay(lr_init, decay_steps, 0.65, staircase=False, name=None)\n",
    "        dqn.compile(Adam(learning_rate =lr), metrics=['mae'])\n",
    "        print(f\"Building {index}: \",'\\n')\n",
    "        dqn.fit(env, nb_steps=self.nb_steps*Nboucle, visualize=False, verbose=1)\n",
    "        print('\\n')\n",
    "\n",
    "            \n",
    "            \n",
    "    def opt(self, name=''):\n",
    "        \"\"\"It allows us to optimize every agents and save the results in a file .h5f to reuse the results.\"\"\"    \n",
    "        #optimization Building 1\n",
    "        self.optBuilding(0.1, 3000, self.Nboucle, 1, self.model1 ,self.dqn1, self.env1)\n",
    "        print()\n",
    "        self.dqn1.save_weights(f'{name}_weights1.h5f', overwrite=True)\n",
    "        \n",
    "        #optimization Building 2\n",
    "        self.model2.load_weights(f'{name}_weights1.h5f') #transfer learning\n",
    "        self.optBuilding(0.1, 3000, self.Nboucle, 2, self.model2 ,self.dqn2, self.env2)\n",
    "        print()\n",
    "        \n",
    "        #optimization Building 3\n",
    "        self.optBuilding(0.1 , 5000, 2*self.Nboucle, 3, self.model3 ,self.dqn3, self.env3)\n",
    "        print()\n",
    "    \n",
    "        self.save(name)\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"This function is testing the three models and return each profitability\"\"\"    \n",
    "    \n",
    "        lr=0.005\n",
    "        self.dqn1.compile(Adam(lr=lr), metrics=['mae'])\n",
    "        self.dqn2.compile(Adam(lr=lr), metrics=['mae'])\n",
    "        self.dqn3.compile(Adam(lr=lr), metrics=['mae'])\n",
    "\n",
    "        \n",
    "        scores = self.dqn1.test(self.env1, visualize=False)\n",
    "        score1 = -np.mean(scores.history['episode_reward'])\n",
    "        \n",
    "        scores = self.dqn2.test(self.env2, visualize=False)\n",
    "        score2 = -np.mean(scores.history['episode_reward'])\n",
    "\n",
    "        scores = self.dqn3.test(self.env3, visualize=False)\n",
    "        score3 = -np.mean(scores.history['episode_reward'])\n",
    "        \n",
    "        return score1, score2, score3\n",
    "                \n",
    "        \n",
    "    def save(self, name=''):\n",
    "        \"\"\"It allows us to save our results with a parameter for the name called 'name '. \"\"\"\n",
    "        self.dqn1.save_weights(f'{name}_weights1.h5f', overwrite=True)\n",
    "        self.dqn2.save_weights(f'{name}_weights2.h5f', overwrite=True)\n",
    "        self.dqn3.save_weights(f'{name}_weights3.h5f', overwrite=True)\n",
    "\n",
    "    \n",
    "    def load(self, name):\n",
    "        \"\"\"We can load the three models with the information 'name' used to save them.\"\"\"\n",
    "        #Loading of models\n",
    "        self.model1.load_weights(f'{name}_weights1.h5f')\n",
    "        self.model2.load_weights(f'{name}_weights2.h5f')\n",
    "        self.model3.load_weights(f'{name}_weights3.h5f')\n",
    "\n",
    "        #Inputs\n",
    "        actions1 = self.env1.action_space.n\n",
    "        actions3 = self.env3.action_space.n\n",
    "        \n",
    "        #Modification of the agents\n",
    "        self.dqn1 = self.build_agent(self.model1, actions1)\n",
    "        self.dqn2 = self.build_agent(self.model2, actions1)\n",
    "        self.dqn3 = self.build_agent(self.model3, actions3)\n",
    "           \n",
    "\n",
    "    def build_model12(self, states, actions):\n",
    "        \"\"\"This function returns the model use for the agent of the building 1 and 2.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,states[0])))\n",
    "        model.add(Dense(60, activation='relu' , input_shape=(1,states[0])))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(actions, activation='linear'))\n",
    "        return model\n",
    "    \n",
    "    def build_model3(self, states, actions):\n",
    "        \"\"\"This function returns the model use for the agent of the building 3.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,states[0])))\n",
    "        model.add(Dense(90, activation='relu' , input_shape=(1,states[0])))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(actions, activation='linear'))\n",
    "        return model\n",
    "    \n",
    "    def build_agent(self,model, actions):\n",
    "        \"\"\"It builds an double DQN agent that move with BoltzmannQPolicy\"\"\"\n",
    "        policy = BoltzmannQPolicy()\n",
    "        memory = SequentialMemory(limit=8760, window_length=1)\n",
    "        dqn = DQNAgent(model=model, enable_double_dqn=True, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=500, target_model_update=1e-6)\n",
    "\n",
    "        return dqn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environments creations\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n",
    "building_environments[0].reset(testing=False)\n",
    "building_environments[1].reset(testing=False)\n",
    "building_environments[2].reset(testing=False)\n",
    "\n",
    "for i in range(3):\n",
    "    building_environments[i].reset(testing=False)\n",
    "    building_environments[i].seed(42)\n",
    "      \n",
    "\n",
    "#Environments notations\n",
    "env1 = building_environments[0]\n",
    "env2 = building_environments[1]\n",
    "env3 = building_environments[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.process_time()\n",
    "\n",
    "#Hyperparameters\n",
    "Nboucle = 7\n",
    "nb_steps = 10000\n",
    "\n",
    "#Creation of the models\n",
    "ModelBuildings = OPT(Nboucle, nb_steps, env1, env2, env3)\n",
    "\n",
    "#Optimization and save of the parameters found\n",
    "ModelBuildings.opt('Train_7_')\n",
    "\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_start = time.process_time()\n",
    "\n",
    "for i in range(3):\n",
    "    building_environments[i].reset(testing=False)\n",
    "    building_environments[i].seed(42)\n",
    "\n",
    "env1 = building_environments[0]\n",
    "env2 = building_environments[1]\n",
    "env3 = building_environments[2]\n",
    "\n",
    "ModelBuildings = OPT(Nboucle, nb_steps, env1, env2, env3)\n",
    "ModelBuildings.load('Train_7_')\n",
    "\n",
    "total_cost = ModelBuildings.test()\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "hiparis",
   "language": "python",
   "name": "hiparis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
